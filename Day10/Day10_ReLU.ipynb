{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 : loss 1.78687608 ; accuracy 0.409\n",
      "Step 20 : loss 1.23370647 ; accuracy 0.5455\n",
      "Step 30 : loss 0.751195729 ; accuracy 0.62\n",
      "Step 40 : loss 0.744434655 ; accuracy 0.67525\n",
      "Step 50 : loss 0.39180249 ; accuracy 0.7092\n",
      "Step 60 : loss 0.398978919 ; accuracy 0.736833334\n",
      "Step 70 : loss 0.384810954 ; accuracy 0.759571433\n",
      "Step 80 : loss 0.283165574 ; accuracy 0.776875\n",
      "Step 90 : loss 0.271611214 ; accuracy 0.790333331\n",
      "Step 100 : loss 0.316256613 ; accuracy 0.8008\n",
      "Step 110 : loss 0.274215162 ; accuracy 0.810090899\n",
      "Step 120 : loss 0.203440234 ; accuracy 0.818333328\n",
      "Step 130 : loss 0.233346179 ; accuracy 0.825692296\n",
      "Step 140 : loss 0.206543267 ; accuracy 0.833214283\n",
      "Step 150 : loss 0.33658573 ; accuracy 0.839\n",
      "Step 160 : loss 0.347389758 ; accuracy 0.843937516\n",
      "Step 170 : loss 0.26370573 ; accuracy 0.848470569\n",
      "Step 180 : loss 0.351972729 ; accuracy 0.852666676\n",
      "Step 190 : loss 0.232818142 ; accuracy 0.855789483\n",
      "Step 200 : loss 0.302266836 ; accuracy 0.8589\n",
      "Final step tf.Tensor(200, shape=(), dtype=int32) : loss tf.Tensor(0.30226684, shape=(), dtype=float32) ; accuracy tf.Tensor(0.8589, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "x = tf.random.uniform((3, 3))\n",
    "y = tf.random.uniform((3, 3))\n",
    "\n",
    "\n",
    "import timeit\n",
    "conv_layer = tf.keras.layers.Conv2D(100, 3)\n",
    "\n",
    "image = tf.zeros([1, 200, 200, 100])\n",
    "\n",
    "lstm_cell = tf.keras.layers.LSTMCell(10)\n",
    "\n",
    "@tf.function\n",
    "def lstm_fn(input, state):\n",
    "  return lstm_cell(input, state)\n",
    "\n",
    "input = tf.zeros([10, 10])\n",
    "state = [tf.zeros([10, 10])] * 2\n",
    "\n",
    "# warm up\n",
    "lstm_cell(input, state); lstm_fn(input, state)\n",
    "\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.models.Model):\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, input_data):\n",
    "    if tf.reduce_mean(input_data) > 0:\n",
    "      return input_data\n",
    "    else:\n",
    "      return input_data // 2\n",
    "\n",
    "\n",
    "model = CustomModel()\n",
    "\n",
    "model(tf.constant([-2, -4]))\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    # Try setting a breakpoint here!\n",
    "    # Example:\n",
    "    #   import pdb\n",
    "    #   pdb.set_trace()\n",
    "    x = x + 1\n",
    "  return x\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "# You can now set breakpoints and run the code in a debugger.\n",
    "f(tf.constant(1))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "\n",
    "def prepare_mnist_features_and_labels(x, y):\n",
    "  x = tf.cast(x, tf.float32) / 255.0\n",
    "  y = tf.cast(y, tf.int64)\n",
    "  return x, y\n",
    "\n",
    "def mnist():\n",
    "  (x, y), _ = tf.keras.datasets.mnist.load_data()\n",
    "  ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "  ds = ds.map(prepare_mnist_features_and_labels)\n",
    "  ds = ds.take(20000).shuffle(20000).batch(100)\n",
    "  return ds\n",
    "\n",
    "train_dataset = mnist()\n",
    "\n",
    "model = tf.keras.Sequential((\n",
    "    tf.keras.layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)))\n",
    "model.build()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "compute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(y, logits)\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  compute_accuracy(y, logits)\n",
    "  return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, optimizer):\n",
    "  train_ds = mnist_dataset()\n",
    "  step = 0\n",
    "  loss = 0.0\n",
    "  accuracy = 0.0\n",
    "\n",
    "  for x, y in train_ds:\n",
    "    step += 1\n",
    "    loss = train_one_step(model, optimizer, x, y)\n",
    "    if step % 10 == 0:\n",
    "      tf.print('Step', step, ': loss', loss, '; accuracy', compute_accuracy.result())\n",
    "  return step, loss, accuracy\n",
    "\n",
    "\n",
    "step, loss, accuracy = train(model, optimizer)\n",
    "print('Final step', step, ': loss', loss, '; accuracy', compute_accuracy.result())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
