# Day 8

<b>Today we will talk about how the <i>Deep Learning</i> begins.</b>



### 1. Start of Deep Learning

***

Many scientists wanted machines to think and solve the difficult problems that human struggled with.

<b>So, they first analyzed the brain.</b>

Scientist found that there is a unit called a neuron consists very simply. So, they calculate the neurons mathematically and produced the result.

<img src="https://user-images.githubusercontent.com/32675267/68006755-fa9aa200-fcbc-11e9-8f80-a7624108dcc2.png" alt="1280px-Neuron svg" style="zoom:25%;" />



The result is the following picture.

It was made in hardware and attracted a lot of attention.

![logi](https://user-images.githubusercontent.com/32675267/68007010-b78cfe80-fcbd-11e9-84b3-ae99796beb44.jpg)

People's interest has deepened as we have found the results for 'And' and 'Or' well. But then problems were found. <b>The result of 'XOR' could not be found.</b>

'And' and 'Or' can be separated linearly, but 'XOR' is impossible.

![or_and_xor](https://user-images.githubusercontent.com/32675267/68007981-a1347200-fcc0-11e9-9519-abfd9b70614c.png)



Not able to solve this problem, the neuron network enters a plateau.

***

In 1974, Dr. Paul Werbos and in 1986, Hinton, came up with the concept of a <b>Backpropagation</b>. This allows for more complex problems, including XOR problems.

![back](https://user-images.githubusercontent.com/32675267/68008249-9c23f280-fcc1-11e9-81a0-a3bba46aa470.PNG)

<b>Backpropagation</b> detects an error and then corrects the error in a backward manner from there.

Also, Convolution Neural Network was developed by Dr.LeKun.

![cnn](https://user-images.githubusercontent.com/32675267/68008756-52d4a280-fcc3-11e9-91e4-bfcc2cc298b2.png)

CNN does not recognize it at once, but instead breaks up the input data and combines the results later.



<B>But in 1995, they found a big problem again.</b>



Backpropagation has been able to solve simple problems, but for the complex problems we are trying to solve, we send back the error that occurred earlier, so the signal of this error becomes so weak that it becomes meaningless later on. In other words, the more you train on complex problems, the lower the performance.

Dr.LeKun also said that other, random-style algorithms are more powerful, and once again the heat of AI has cooled.



### 2. CIFAR and 'DEEP'

***

And there is an organization that appeared at this time.

<b>CIFAR</b> is a Canadian organization that has conducted <b>Deep Learning</b> research.

***

In 2007,  a new solution was founded.

Until now, the reason why <b>Deep Leaning</b> was not done properly was because of the initial value. Setting this initial value correctly will solve a deeper problem.

And at this time, it was renamed <b>Deep Learning</b>, not Neural Nets.

***

And <b>Deep Learning</b> started getting more and more attention with ImageNet, which looked at the images and tailored what they were.

In 2015, <b>Deep Learning</b>'s  error rate is 3%, and people's error rate is 5%.

And now, the technology has been developed to describe the image, not just to fit the image.



### 3. Reference

***

https://www.youtube.com/watch?v=n7DNueHGkqE&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=23&t=0s
