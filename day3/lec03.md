# [lec03]linear regression cost minimization
we will learn about how to minimize the cost function of Linear Regression model <i>Cost(W,b)</i>
***

​							<img src="https://github.com/teddy309/10-Days-Of-DL/blob/master/day3/images/lec03funcsimplify.PNG" style="zoom:40%;"/> 

### 1) Condition Precedent
> <b>Convex Function</b>
​							<img src="https://github.com/teddy309/10-Days-Of-DL/blob/master/day3/images/lec03funcsimplify.PNG" style="zoom:40%;"/> 
​							<img src="https://github.com/teddy309/10-Days-Of-DL/blob/master/day3/images/lec03funcsimplify.PNG" style="zoom:40%;"/> 
We have to check if cost function is in convex figure.
At cost function like left side, we cannot check minimum point of function by gradient descent. 
So the function have to be in figure like right side.
---

> <b>Convex Graph</b>

​							<img src="https://github.com/teddy309/10-Days-Of-DL/blob/master/day3/images/lec03funcsimplify.PNG" style="zoom:40%;"/> 
To minimize the cost function, we want to know  which point(W,b) can minimize the cost function.
We will simplify the Hypothesis Hx for concentrating on W value of it.
​							<img src="https://github.com/teddy309/10-Days-Of-DL/blob/master/day3/images/lec03funcsimplify.PNG" style="zoom:40%;"/> 


### 2) Gradient Descent


---
### 3) Differentiation(미분)

This is a method of progressing learning <u>_without a label for data_</u>.

It is often used to discover hidden features or structures of data.



> <b>Clustering</b>
>
> This is a method of grouping data with similar characteristics.
